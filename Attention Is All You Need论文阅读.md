

# Attention Is All You Need论文阅读

## 摘要

目前主流的序列转导模型基于复杂的循环神经网络或卷积神经网络，这些模型包含**编码器和解码器**。性能最佳的模型还通**过注意力机制连接编码器和解码器**。我们提出了一种**新的简单网络架构——Transformer**，**该架构仅基于注意力机制，完全摒弃了递归和卷积**。在两个机器翻译任务上的实验表明，这些模型在质量上更优，同时更具并行性，且训练时间显著减少。我们的模型在WMT 2014英语-德语翻译任务中达到了28.4的BLEU分数，比现有最佳结果（包括集成模型）提高了超过2个BLEU分数。在WMT 2014英语-法语翻译任务中，我们的模型在8个GPU上训练3.5天后，达到了41.8的新单模型最先进BLEU分数，而这一训练成本仅为文献中最佳模型的一小部分。我们通过将其成功应用于英语构成句法分析（无论训练数据量大小）展示了Transformer的良好泛化能力。



## 1 引言

循环神经网络（尤其是长短时记忆网络 [13] 和门控循环神经网络 [7]）已被确立为语言建模和机器翻译等序列建模与转导问题的最先进方法 [35, 2, 5]。此后，许多研究继续推动循环语言模型和编码器-解码器架构的边界 [38, 24, 15]。

**循环模型通常沿着输入和输出序列的符号位置分解计算**。通过将位置与计算时间的步骤对齐，它们**生成一系列隐藏状态$$h_t$$，这些状态是前一隐藏状态 $$h_{t−1}$$ 和位置 $$t $$输入的函数**。这种本质上**顺序的性质阻碍了训练样本内的并行化，而这一点在处理较长序列时变得尤为重要，因为内存限制会限制跨样本的批量处理**。尽管最近的工作通过分解技巧 [21] 和条件计算 [32] 在计算效率方面取得了显著提升，但顺序计算的基本限制仍然存在。

注意力机制已成为各种任务中具有吸引力的序列建模和转导模型的重要组成部分，允许在不考虑输入或输出序列中依赖关系距离的情况下进行建模 [2, 19]。然而，在几乎所有情况下 [27]，这些注意力机制都与循环网络结合使用。

在本研究中，我们提出了 Transformer，**一种完全摒弃递归的模型架构，而是完全依赖注意力机制来在输入和输出之间建立全局依赖关系。Transformer允许显著的并行化，并且在仅使用八个 P100 GPU 训练十二小时后，即可达到翻译质量的新最先进水平。**



## 2 背景

减少顺序计算的目标也构成了Extended Neural GPU [16]、ByteNet [18]和ConvS2S [9]的基础，这些模型均使用卷积神经网络作为基本构建块，为所有输入和输出位置并行计算隐藏表示。在这些模型中，将两个任意输入或输出位置的信号相关联所需的运算量与它们之间的距离成正比增长，ConvS2S为线性增长，而ByteNet为对数增长。这使得学习远距离位置之间的依赖关系更加困难 [12]。在Transformer中，这种运算量被减少为一个常数，尽管由于对注意力加权位置进行平均而导致有效分辨率降低，但我们在第3.2节中通过多头注意力（Multi-Head Attention）来抵消这一影响。

**自注意力（Self-attention），有时也称为内注意力（intra-attention），是一种注意力机制，用于将单个序列的不同位置相关联，从而计算该序列的表示**。自注意力已在多种任务中成功应用，包括阅读理解、摘要生成、文本蕴含和学习任务无关的句子表示 [4, 27, 28, 22]。

端到端记忆网络（End-to-end memory networks）基于一种递归注意力机制，而非序列对齐的递归，已被证明在简单语言的问题回答和语言建模任务中表现良好 [34]。

然而，据我们所知，Transformer是第一个完全依赖自注意力来计算输入和输出表示的转导模型，而无需使用序列对齐的RNN或卷积。在以下部分中，我们将描述Transformer，解释自注意力的作用，并讨论其相对于[17, 18]和[9]等模型的优势。



## 3 模型架构

目前最具有竞争力的神经序列转导模型大多采用编码器-解码器结构 [5, 2, 35]。在此结构中，**编码器将输入符号表示序列 $$(x₁, ..., xₙ) $$映射为连续表示序列 $$z = (z₁, ..., zₙ)$$。给定 $z$，解码器随后逐个生成输出符号序列$ (y₁, ..., yₘ)$**。在每一步，模型都是自回归的 [10]，即在生成下一个符号时，会将之前生成的符号作为额外输入。

Transformer采用了这种总体架构，其**编码器和解码器分别由堆叠的自注意力层和点式全连接层组成**，如图1的左半部分和右半部分所示。

![image-20250724152029632](http://image.huawei.com/tiny-lts/v1/images/hi3ms/071c996ef9823c2ff51a995f21aafb58_600x919.png)



### 3.1 编码器和解码器堆叠

**编码器**：**编码器由N=6个相同的层堆叠而成**。每一层包含两个子层。**第一个子层是一个多头自注意力机制（multi-head self-attention mechanism），第二个子层是一个简单的、基于位置的全连接前馈网络（position-wise fully connected feed-forward network）**。我们在每个子层周围采用了**残差连接（residual connection）[11]，并在其后应用了层归一化（layer normalization）[1]**。也就是说，每个子层的输出为：
$$LayerNorm(x + Sublayer(x))$$，其中$$Sublayer(x)$$ 是子层自身实现的函数。为了便于这些残差连接，模型中的所有子层以及嵌入层的输出维度均为 $$d_{model} = 512$$

**解码器**：**解码器也由N=6个相同的层堆叠而成**。与编码器层的两个子层不同，**解码器在每个层中额外插入了一个第三子层**，该子层对**编码器堆栈的输出执行多头注意力（multi-head attention）**。与编码器类似，我们在每个子层周围应用了残差连接，随后进行层归一化。我们还**对解码器堆栈中的自注意力子层进行了修改，以防止位置关注后续位置（即未来位置）**。**这种掩码（masking）机制，结合输出嵌入偏移一个位置的事实，确保了位置$i$的预测只能依赖于位置$i$之前已知的输出。**



### 3.2 注意力机制

**注意力函数可以描述为将一个查询（query）和一组键值对（key-value pairs）映射到一个输出，其中查询、键、值和输出都是向量**。输出是通过加权求和这些值计算得到的，其中**每个值的权重由查询与对应键之间的兼容性函数（compatibility function）计算得**出。

![image-20250724153711365](http://image.huawei.com/tiny-lts/v1/images/hi3ms/5a3ed9db7c932a0260a38740cef2c048_1089x616.png)

#### 3.2.1 缩放点积注意力（Scaled Dot-Product Attention）

我们称我们的**特定注意力机制为“缩放点积注意力”（Scaled Dot-Product Attention，如图2所示）**。输入包括维度为 ($d_k$) 的查询（$query$）和键（$key$），以及维度为 ($d_v$) 的值（$value$）。我们计**算查询与所有键的点积，将每个点积结果除以 ($\sqrt{d_k}$)，然后应用 $softmax $函数以获得对值的权重**。

在实际操作中，我们同时对**一组查询计算注意力函数，并将它们打包成一个矩阵 ($Q$)。键和值也被打包成矩阵 ($K$) 和 ($V$)**。我们计算输出矩阵如下：
$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V \quad (1)
$$


最常用的两**种注意力函数是加性注意力（additive attention）[2] 和点积（乘性）注意力（dot-product attention）**。**点积注意力与我们的算法完全相同，只是缺少了缩放因子$\frac{1}{\sqrt{d_k}}$**。**加性注意力使用一个带有单个隐藏层的前馈网络来计算兼容性函数**。尽管在理**论复杂度上两者相似，但点积注意力在实践中快得多且更节省空间，因为它可以利用高度优化的矩阵乘法代码实现**。

然而，**对于较小的 ($d_k$) 值，两种机制表现相似；但对于较大的 ($d_k$) 值，加性注意力优于未缩放的点积注意力 [3]。我们推测，当 ($d_k$) 较大时，点积的值会变得很大，将$ softmax$ 函数推入梯度极小的区域。为了抵消这种影响，我们将点积结果除以 ($\sqrt{d_k}$)。**



#### 3.2.2 多头注意力（Multi-Head Attention）

与使用单一的 ($d_{\text{model}}$)-维键、值和查询进行注意力计算不同，我们发现**将查询、键和值分别通过 ($h$) 个不同的线性投影（learned linear projections）映射到 ($d_k$)、($d_k$) 和 ($d_v$) 维度是有益的**。然后，我们对这些**投影后的查询、键和值并行地执行注意力函数，得到 ($d_v$)-维的输出值。这些输出值被拼接（concatenated）后，再进行一次线性投影，得到最终的输出值，如图2所示**。

**多头注意力（Multi-Head Attention）允许模型在不同位置同时关注来自不同表示子空间的信息**。使用**单头注意力时，平均操作会抑制这种多方面的关注能力**。具体来说，多头注意力的计算公式如下：
$$
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \dots, \text{head}_h) W_O
$$
其中，**每个头（head）的计算为**：
$$
\text{head}_i = \text{Attention}(Q W_i^Q, K W_i^K, V W_i^V)
$$
这里的**投影矩阵为**：
$$
W_i^Q \in \mathbb{R}^{d_{\text{model}} \times d_k}, \quad W_i^K \in \mathbb{R}^{d_{\text{model}} \times d_k}, \quad W_i^V \in \mathbb{R}^{d_{\text{model}} \times d_v}
$$
以及**最终的投影矩阵**：
$$
W_O \in \mathbb{R}^{h d_v \times d_{\text{model}}}
$$
在本研究中，我们采用**了 ($h = 8$) 个并行的注意力层（即8个头）。对于每个头，我们使用 ($d_k = d_v = d_{\text{model}} / h = 64$)。由于每个头的维度降低，总计算成本与单头注意力（使用完整维度）相当**。

**多头注意力通过并行计算多个注意力头，使得模型能够从不同的子空间中捕获多样化的信息，从而增强了模型的表达能力和灵活性**。

#### 3.2.3 我们模型中注意力的应用

Transformer模型中，多头注意力机制以三种不同的方式应用：

##### 1. **编码器-解码器注意力（Encoder-Decoder Attention）**

在编码器-解码器注意力层中，**查询（queries）来自解码器的上一层，而键（keys）和值（values）来自编码器的输出**。**这使得解码器中的每个位置都可以关注输入序列的所有位置**。这种机制模仿了传统序列到序列模型（如[38, 2, 9]）中典型的编码器-解码器注意力机制。通过这种方式，解码器能够利用编码器对输入序列的整体理解，从而生成更准确的输出。

##### 2. **编码器的自注意力（Encoder Self-Attention）**

**编码器包含自注意力层**。在**自注意力层中，所有的键、值和查询都来自同一个地方，即编码器上一层的输出**。这意味着**编码器中的每个位置都可以关注上一层编码器的所有位置。这种机制允许编码器捕捉输入序列中全局的依赖关系，从而生成更丰富的上下文表示**。

##### 3. **解码器的自注意力（Decoder Self-Attention）**

与编码器类似，**解码器的自注意力层允许解码器中的每个位置关注解码器中所有位置（包括当前位置）**。然而，**为了保持自回归性质（即每个位置只能依赖于之前生成的位置），我们需要防止信息从未来位置流向当前位置**。我们通过**在缩放点积注意力中对非法连接的位置进行掩码（即将这些位置的值设为负无穷）来实现这一点**。这样，在 softmax 函数中，这些位置的权重将为零，从而确保解码器的自注意力只能关注当前位置及之前的位置。

### 3.3 位置前馈网络（Position-wise Feed-Forward Networks）

在Transformer模型中，除了注意力子层外，编码器和解码器的每一层都包含一个位置**前馈网络（Position-wise Feed-Forward Network，FFN**）。**这个网络对序列中的每个位置独立地应用相同的全连接前馈操作，从而为模型引入非线性变换，增强其表达能力。**

**结构与计算**

**位置前馈网络由两个线性变换和一个ReLU激活函数组成**，具体计算过程如下：
$$
\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2 \quad (2)
$$
其中：

- ($x$) 是输入向量。
- ($W_1$) 和 ($W_2$) 是权重矩阵。
- ($b_1$) 和 ($b_2$) 是偏置向量。
- ($\max(0, \cdot)$) 是ReLU激活函数，用于引入非线性。

**维度与参数**

- **输入和输出维度**：输入和输出的维度均为 ($d_{\text{model}} = 512$)，确保与注意力子层的输出兼容。
- **中间层维度**：中间层的维度为 ($d_{\text{ff}} = 2048$)，较大的中间层维度有助于模型学习更复杂的特征。

**参数共享与独立性**

虽然**线性变换在不同位置上是相同的，但每个层的前馈网络都有自己的权重和偏置，与其他层的前馈网络不同**。这意味着每个层的前馈网络独立学习特征，增强了模型的表达能力。

**与卷积的类比**

位置前馈网络可以被描述为两个核大小为1的卷积操作。这是因为每个位置的处理是独立的，类似于对每个位置应用一个1x1的卷积核。这种结构不仅保持了并行计算的优势，还允许模型在每个位置上进行复杂的非线性变换。

**作用与意义**

位置前馈网络在Transformer模型中起到了关键作用：

1. **引入非线性**：通过ReLU激活函数，模型能够学习更复杂的模式和特征。
2. **增强表达能力**：较大的中间层维度使得模型能够捕捉更丰富的信息。
3. **保持位置独立性**：每个位置独立处理，确保模型能够并行计算，同时通过注意力机制捕捉全局依赖关系。

**应用方式**

位置前馈网络在编码器和解码器中的应用方式相同，每个层都包含相同的结构，只是参数不同。这种一致性确保了模型的统一性和稳定性。



### 3.4 嵌入层与Softmax层(Embeddings and Softmax )

在Transformer模型中，嵌入层和Softmax层的设计对于模型的整体性能和训练效率起到了关键作用。以下是对其详细解释：

**1. 嵌入层（Embedding Layer）**

嵌入层的作用是**将输入和输出的离散词或字符转换为连续的向量表示**。具体来说：

- **输入嵌入（Input Embedding）**：**将输入的词或字符映射到一个$d_{model}$维的向量空间。每个词通过一个可学习的嵌入矩阵（embedding matrix）进行转换**。
- **输出嵌入（Output Embedding）**：同样地，**输出的词也被映射到$d_{model}$维的向量空间，以便与解码器的输出进行对齐**。

在嵌入层中，**权重矩阵被乘以$\sqrt{d_{model}}$。这种缩放操作有助于平衡嵌入向量的范数，防止梯度爆炸或消失，从而保持训练过程的稳定性**。

**2. Softmax层**

Softmax层的作用是**将解码器的输出转换为预测下一个词的概率分布**。具体步骤如下：

- **线性变换（Linear Transformation）**：**解码器的输出经过一个线性变换，将$d_{model}$维的向量映射到词表的维度（例如30000）。**
- **Softmax函数（Softmax Function）**：**对线性变换的结果应用Softmax函数，得到每个词的概率分布。**

**3. 权重共享（Weight Sharing）**

为了减少模型的参数数量并提高训练效率，Transformer模型在嵌入层和Softmax层之间进行了权重共享：

- **输入嵌入与输出嵌入共享权重**：**输入和输出嵌入层使用相同的权重矩阵，这意味着它们共享相同的参数**。这不仅减少了模型的参数数量，还迫使模型学习到更通用的词表示。
- **预Softmax线性变换与嵌入层共享权重**：**预Softmax的线性变换与输入嵌入层共享权重矩阵**。具体来说，**输出层的权重矩阵是输入嵌入矩阵的转置**。这种设计不仅减少了参数数量，还保持了输入和输出嵌入的一致性。

**4. 缩放操作（Scaling）**

在嵌入层中，**权重矩阵被乘以$\sqrt{d_{model}}$。这种缩放操作有助于平衡嵌入向量的范数，防止梯度爆炸或消失，从而保持训练过程的稳定性**。

**5. 动机与影响**

- **权重共享的动机**：通**过共享权重，模型可以利用输入和输出之间的对称性，减少参数数量，提高训练效率，并降低过拟合的风险**。
- **缩放操作的影响**：**缩放操作有助于保持梯度稳定，防止高维空间中向量范数过大导致的梯度爆炸问题**。

**6. 总结**

嵌入层和Softmax层在Transformer模型中起到了关键作用：

- **嵌入层**：将离散的词或字符转换为连续的向量表示，通过缩放操作保持梯度稳定。
- **Softmax层**：将解码器的输出转换为概率分布，用于预测下一个词。
- **权重共享**：减少模型参数数量，提高训练效率，同时保持输入和输出嵌入的一致性。

### 3.5 位置编码（Positional Encoding）

在Transformer模型中，位置编码（Positional Encoding）是至关重要的组件，用于为模型提供序列中每个词的位置信息。**由于Transformer模型不使用循环神经网络（RNN）或卷积神经网络（CNN），它无法像这些模型那样自然地处理序列中的顺序信息。因此，必须引入位置编码机制，以使模型能够利用序列中各个位置的顺序信息**。

**位置编码被添加到编码器和解码器堆栈底部的输入嵌入中**。这些**位置编码与嵌入具有相同的维度 ($d_{\text{model}}$)，因此可以相加**。位置编码有多种选择，包括学习的和固定的。

在本工作中，使用了**不同频率的正弦和余弦函数来生成位置编码**。具体来说，对于位置 ($pos$) 和维度 ($i$)，位置编码的计算公式如下：
$$
\text{PE}(pos, 2i) = \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
$$

$$
\text{PE}(pos, 2i+1) = \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
$$

这里，($pos$) 是位置，($i$) 是维度。也就是说，**位置编码的每个维度对应一个正弦波**。**波长从 ($2\pi$) 到 ($10000 \cdot 2\pi$) 形成一个几何级数**。**选择这种函数的原因是假设它允许模型轻松地通过相对位置进行注意力机制，因为对于任何固定的偏移 ($k$)，($\text{PE}*{pos+k}$) 可以表示为 ($\text{PE}*{pos}$) 的线性函数**。

此外，还**尝试使用学习的位置嵌入，发现两种版本的结果几乎相同。选择正弦版本是因为它可能允许模型外推到比训练中遇到的更长的序列长度**。

**位置编码的作用是为模型提供序列中每个词的位置信息，使其能够处理顺序信息。通过使用不同频率的正弦和余弦函数，模型可以有效地编码位置信息，并且具有外推到更长序列的能力**。

在实际应用中，位置编码的选择可能会影响模型的性能和训练效果。因此，理解位置编码的设计和作用对于优化Transformer模型非常重要。

![image-20250724170334680](http://image.huawei.com/tiny-lts/v1/images/hi3ms/11ddbdb0968f034ecbcecf7bc89c6de7_1107x870.png)



## 4 为什么选择自注意力机制

在本节中，我们将**自注意力层与通常用于将一个可变长度符号表示序列 ($(x_1, \dots, x_n)$) 映射到另一个等长序列 ($(z_1, \dots, z_n)$)（其中 ($x_i, z_i \in \mathbb{R}^d$)）的循环层和卷积层进行比较，例如典型序列转导编码器或解码器中的隐藏层**。我们选择自注意力机制的动机基于以下三个目标。

第一个目标是**每层的总计算复杂度**。第二个目标是可以**并行化的计算量，具体通过所需的最小顺序操作数量来衡量**。第三个目标是**网络中长距离依赖之间的路径长度**。**学习长距离依赖是许多序列转导任务中的关键挑战**。影响学习这种依赖关系能力的一个**关键因素是前向和后向信号在网络中必须遍历的路径长度**。**输入和输出序列中任意组合的位置之间的路径越短，学习长距离依赖就越容易** [12]。因此，我们还比较了由不同类型的层组成的网络中任意两个输入和输出位置之间的最大路径长度。

如表 1 所示，自注意力层通过一个固定数量的顺序执行操作连接所有位置，而循环层需要 ($O(n)$) 个顺序操作。**在计算复杂度方面，当序列长度 ($n$) 小于表示维度 ($d$) 时（这在机器翻译中使用词块 [38] 和字对 [31] 表示的状态-of-the-art 模型中最为常见），自注意力层比循环层更快**。为了提高涉及非常长序列的任务的计算性能，可以**将自注意力限制为仅考虑输入序列中以相应输出位置为中心的大小为 (r) 的局部区域。这将使最大路径长度增加到 ($O(n/r)$)**。我们计划在未来进一步研究这种方法。

单个卷积层（核宽度 ($k < n$)）无法连接所有输入和输出位置的对。要做到这一点，需要堆叠 ($O(n/k)$) 个卷积层（对于连续核的情况），或 ($O(\log_k(n))$) 个卷积层（对于扩张卷积 [18] 的情况），从而增加网络中任意两个位置之间的最长路径长度。一般来说，卷积层比循环层更昂贵，昂贵程度与 (k) 成正比。然而，可分离卷积 [6] 可以显著降低复杂度，使其达到 ($O(k \cdot n \cdot d + n \cdot d^2)$)。然而，即使 (k = n)，可分离卷积的复杂度也与自注意力层和点式前馈层的组合相当，这是我们模型中采用的方法。

此外，自注意力机制可以生成更易于解释的模型。我们检查了模型的注意力分布，并在附录中展示了和讨论了示例。不仅单个注意力头清晰地学习执行不同的任务，许多注意力头的行为似乎与句子的句法和语义结构相关。

![image-20250724172049694](http://image.huawei.com/tiny-lts/v1/images/hi3ms/8335ab92ee203fc2d21424de3dc57441_1299x414.png)

**1. 计算复杂度**

- **自注意力层**：自注意力层的计算复杂度为 ($O(n^2 d)$)，其中 ($n$) 是序列长度，($d$) 是表示维度。在处理句子表示时，当序列长度 ($n$) 小于表示维度 ($d$)（如机器翻译中的词块和字对表示），自注意力层的计算速度比循环层更快。
- **循环层**：循环层的计算复杂度为 ($O(n d^2)$)，随着序列长度 ($n$) 的增加，计算成本显著上升。
- **卷积层**：卷积层的计算复杂度取决于卷积核的大小 ($k$)。单个卷积层无法连接所有输入和输出位置，需要堆叠多个卷积层，这会增加网络中任意两个位置之间的最长路径长度。

**2. 并行计算能力**

- **自注意力层**：自注意力层可以在 ($O(1)$) 的顺序操作中处理整个序列，这意味着它可以完全并行化，极大地提高了计算效率。
- **循环层**：循环层需要 ($O(n)$) 的顺序操作，这限制了并行计算的能力，因为每个时间步的计算依赖于前一个时间步的结果。
- **卷积层**：虽然卷积层可以部分并行化，但其计算成本通常比循环层高，尤其是当卷积核较大时。然而，可分离卷积（Separable Convolutions）可以显著降低复杂度。

**3. 长距离依赖的路径长度**

- **自注意力层**：**自注意力层通过直接连接所有位置，使得长距离依赖的路径长度保持在 ($O(1)$)**。这意味着模型可以轻松捕捉到序列中任意位置之间的依赖关系，这对于处理长距离依赖关系尤为重要。
- **循环层和卷积层**：循环层和卷积层的路径长度随着序列长度的增加而增加。循环层需要逐个处理序列中的每个位置，而卷积层则需要堆叠多个层来连接所有位置，这会增加网络中任意两个位置之间的最长路径长度。

**4. 可解释性**

自注意力机制的一个额外好处是生成更可解释的模型。通过检查注意力分布，可以发现不同的注意力头学习执行不同的任务，许多头的行为与句子的句法和语义结构相关。这种可解释性使得模型不仅在性能上表现出色，还能够提供对模型决策过程的洞察。

**5. 处理长序列的优化**

对于涉及非常长序列的任务，可以通过限制自注意力机制只关注输入序列中与输出位置相关的局部区域，以提高计算性能。这将增加最大路径长度到 ($O(n/r)$)，其中 ($r$) 是局部区域的大小。这种方法在保持模型性能的同时，显著提高了计算效率。

**总结**

自注意力机制在Transformer模型中的采用，主要是因为它在**计算复杂度、并行计算能力、长距离依赖的路径长度以及模型可解释性等方面优于传统的循环层和卷积层。通过直接连接所有位置，自注意力层能够高效捕捉长距离依赖关系，同时支持高度并行化计算，这对于处理复杂的自然语言处理任务至关重要**。



## 5 训练

### 5.1 训练数据与批处理

我们使用标准的 WMT 2014 英德双语数据集进行训练，该数据集包含约 450 万个句子对。**句子使用字对编码（byte-pair encoding）[3] 进行编码**，共享的源-目标词汇表包含约 37,000 个词块。对于英法翻译任务，我们使用了规模大得多的 WMT 2014 英法数据集，该数据集包含 3600 万个句子，并将词块分割成一个包含 32,000 个词块的词汇表 [38]。句子对按近似长度分批处理，每个训练批包含一组句子对，其中大约包含 25,000 个源词和 25,000 个目标词。

### 5.2 硬件与训练计划

我们在一台配备 **8 块 NVIDIA P100 图形处理器（GPU）的机器上训练模型**。对于使用本文所述超参数的基线模型，**每个训练步骤大约需要 0.4 秒。我们总共训练基线模型 100,000 个步骤，即 12 小时**。对于较大的模型（如表 3 最后一行所示），每个步骤需要 1.0 秒。较大的模型训练了 300,000 个步骤（约 3.5 天）。

### 5.3 优化器

我们使用 Adam 优化器 [20]，设置参数为 $β₁ = 0.9，β₂ = 0.98$ 和 $ϵ = 10⁻⁹$。我们在训练过程中调整学习率，按照以下公式：
$$
\text{lrate} = d_{\text{model}}^{-0.5} \cdot \min(\text{step_num}^{-0.5}, \text{step_num} \cdot \text{warmup_steps}^{-1.5})
$$
这对应于在前 $warmup_{steps}$ 个训练步骤中线性增加学习率，之后按步数的平方根递减。我们设置 $warmup_steps = 4000$。



### 5.4 正则化

在训练过程中，我们采用了三种正则化方法：

#### 残差Dropout

我们对**每个子层的输出应用Dropout [33]**，在将其添加到**子层输入并进行归一化之前**。此外，我们在**编码器和解码器堆栈中对嵌入和位置编码的和也应用了Dropout**。对于基线模型，我们使用了$P_{drop} = 0.1$的Dropout率。

#### 标签平滑

在训练过程中，我们采用了值为$ϵ_{ls} = 0.1$的标签平滑 [36]。这会降低困惑度，因为模型学会了更加不确定，但提高了准确率和BLEU分数。



## 6 结果

### 6.1 机器翻译

在 WMT 2014 英语到德语的翻译任务中，较大的 Transformer 模型（表 2 中的 Transformer (big)）比之前报告的最佳模型（包括集成模型）高出 2.0 个 BLEU 分数，达到了新的最先进 BLEU 分数 28.4。该模型的配置列在表 3 的最后一行。在 8 块 P100 GPU 上的训练时间为 3.5 天。即使我们的基线模型也超越了所有之前发表的模型和集成模型，而训练成本仅为竞争模型的很小一部分。

在 WMT 2014 英语到法语的翻译任务中，我们的大模型实现了 41.0 的 BLEU 分数，超越了所有之前发表的单一模型，而训练成本仅为前最先进模型的 1/4。用于英语到法语训练的 Transformer (big) 模型使用了 Pdrop = 0.1 的 dropout 率，而不是 0.3。

对于基线模型，我们使用了通过平均最后 5 个检查点（每隔 10 分钟写入一次）获得的单一模型。对于大模型，我们平均了最后 20 个检查点。我们使用了 beam search，设置 beam 大小为 4，长度惩罚因子 α = 0.6 [38]。这些超参数是在开发集上经过实验后选择的。我们在推理过程中将最大输出长度设置为输入长度 + 50，但尽可能提前终止 [38]。

表 2 总结了我们的结果，并将我们的翻译质量和训练成本与文献中其他模型架构进行了比较。我们通过将训练时间、使用的 GPU 数量以及每块 GPU 的持续单精度浮点计算能力估计值相乘，来估算训练模型所使用的浮点运算次数。

![image-20250724173233309](http://image.huawei.com/tiny-lts/v1/images/hi3ms/bf35b96231d7185d69f4c3c5981ecdb7_1256x599.png)

### 6.2 模型变体

为了评估 Transformer 不同组件的重要性，我们在基线模型的基础上进行了多种修改，并在英德翻译任务的开发集 newstest2013 上测量性能变化。我们使用了上一节中描述的 beam search，但没有进行检查点平均。我们在表 3 中展示了这些结果。

**表 3 行 (A)：注意力头数和维度的调整**

在表 3 的行 (A) 中，我们改变了注意力头的数量以及注意力键和值的维度，同时保持计算量不变，如第 3.2.2 节所述。单头注意力比最佳设置低 0.9 个 BLEU 分数，而过多的注意力头也会导致性能下降。

**表 3 行 (B)：注意力键尺寸的影响**

在表 3 的行 (B) 中，我们发现减少注意力键的尺寸 ($d_k$) 会降低模型质量。这表明确定兼容性并不容易，使用比点积更复杂的兼容性函数可能有益。此外，在行 (C) 和 (D) 中，我们观察到更大的模型表现更好，而 dropout 在避免过拟合方面非常有用。

**表 3 行 (E)：位置编码的替换**

在行 (E) 中，我们将正弦位置编码替换为学习的位置嵌入 [9]，发现结果与基线模型几乎相同。这表明位置编码的选择对模型性能影响不大，但正弦编码在训练时可能更稳定。

**总结**

通过这些模型变体的实验，我们发现注意力头数和键尺寸对模型性能有显著影响，更大的模型通常表现更好，而 dropout 是防止过拟合的有效手段。此外，位置编码的选择对模型性能影响较小，但正弦编码在训练时可能更稳定。这些发现为我们进一步优化 Transformer 模型提供了有价值的指导。

![image-20250724173442303](http://image.huawei.com/tiny-lts/v1/images/hi3ms/caf090070926b918fb57f7d187f6e0e2_1311x1017.png)



### 6.3 英语句法分析

为了评估 Transformer 是否能够推广到其他任务，我们在英语句法分析任务上进行了实验。该任务具有特定的挑战：输出受到强烈的结构约束，并且输出长度远长于输入。此外，基于 RNN 的序列到序列模型在小数据集上尚未能达到最先进水平 [37]。

我们在 Penn Treebank 的 Wall Street Journal (WSJ) 部分上训练了一个 4 层的 Transformer 模型，设置 ($d_{\text{model}} = 1024$)，该数据集包含约 40,000 个训练句子。我们还将其用于半监督设置，使用了更大的高置信度和 BerkleyParser 语料库，包含约 17,000,000 个句子 [37]。对于仅使用 WSJ 的情况，我们使用了 16,000 个词块的词汇表；对于半监督设置，我们使用了 32,000 个词块的词汇表。

我们仅在 Section 22 开发集上进行了少量实验，以选择 dropout（包括注意力 dropout 和残差 dropout，见第 5.4 节）、学习率和 beam 大小，其余参数均保持与英德翻译基线模型一致。在推理过程中，我们将最大输出长度增加到输入长度 + 300。对于 WSJ 仅和半监督设置，我们均使用了 beam 大小为 21 和 $α = 0.3$。

表 4 的结果显示，尽管没有进行特定任务的调优，我们的模型表现仍然非常出色，优于之前所有报告的模型，仅在 Recurrent Neural Network Grammar [8] 之下。

与基于 RNN 的序列到序列模型 [37] 不同，即使仅在包含 40,000 个句子的 WSJ 训练集上进行训练，Transformer 也优于 BerkeleyParser [29]。

**总结**

在英语句法分析任务中，Transformer 模型展现了强大的泛化能力。尽管没有进行特定任务的调优，其表现依然优于之前的所有模型，仅在 Recurrent Neural Network Grammar 之下。此外，与基于 RNN 的模型不同，Transformer 即使在小规模数据集上也能够超越传统的 BerkeleyParser。这表明 Transformer 在处理结构化输出和长序列任务中具有显著优势，进一步证明了其在自然语言处理任务中的广泛适用性。

### 7 结论

在本研究中，我们提出了 Transformer，这是第一个完全基于注意力机制的序列转导模型，取代了编码器-解码器架构中最常用的循环层，转而采用多头自注意力机制。

对于翻译任务，Transformer 的训练速度显著快于基于循环或卷积层的架构。在 WMT 2014 英德和英法翻译任务中，我们达到了新的最先进水平。在英德翻译任务中，我们的最佳模型甚至超过了所有之前报告的集成模型。

我们对基于注意力模型的未来充满期待，并计划将其应用于其他任务。我们计划将 Transformer 扩展到涉及文本以外的输入和输出模态的问题，并研究局部受限注意力机制，以高效处理大输入输出，如图像、音频和视频。减少生成过程的顺序性是我们的另一个研究目标。

我们用于训练和评估模型的代码可在 https://github.com/tensorflow/tensor2tensor 获取。

**致谢**
我们感激 Nal Kalchbrenner 和 Stephan Gouws 的富有成效的评论、校正和启发。



## 说明

### **为什么缩放点积注意力中需要除以$\sqrt{d_k}$？**

当维度$d_k$较大时，点积的值会变得很大，这会导致softmax函数进入梯度极小的区域，从而影响模型的训练效果。为了应对这个问题，他们提出将点积结果除以$\sqrt{d_k}$。

假设我们有两个向量$q$和$k$，它们的维度都是$d_k$。如果$d_k=1$，那么$q$和$k$都是标量，点积就是它们的乘积。如果$d_k=2$，那么$q$和$k$都是二维向量，点积就是$q₁k₁ + q₂k₂$。如果$d_k=3$，点积就是$q₁k₁ + q₂k₂ + q₃k₃$，以此类推。

现在，假设$q$和$k$的每个分量都是独立的随机变量，**均值为0，方差为1**。那么，点积$q·k$的期望值是多少呢？根据期望的线性性质，$E[q·k] = E[q₁k₁ + q₂k₂ + ... + q_{d_k} k_{d_k}] = E[q₁k₁] + E[q₂k₂] + ... + E[q_{d_k} k_{d_k}]$。因为$q$和$k$的分量是独立的，所以$E[qᵢkᵢ] = E[qᵢ]E[kᵢ] = 0*0 = 0$。因此，点积的期望值为0。

那么，点积的方差是多少呢？方差$Var(q·k) = Var(q₁k₁ + q₂k₂ + ... + q_{d_k} k_{d_k})$。因为q和k的分量是独立的，所以协方差为0，方差可以分解为各个分量方差的和。即$Var(q·k) = Var(q₁k₁) + Var(q₂k₂) + ... + Var(q_{d_k} k_{d_k})$。

现在，计算$Var(qᵢkᵢ)$。因为$qᵢ$和$kᵢ$都是均值为0，方差为1的独立随机变量，所以$Var(qᵢkᵢ) = E[(qᵢkᵢ)^2] - (E[qᵢkᵢ])^2 = E[qᵢ²kᵢ²] - 0 = E[qᵢ²]E[kᵢ²] = 1*1 = 1$。因此，$Var(q·k) = d_k * 1 = d_k$。

所以，**点积$q·k$的期望值为0，方差为$d_k$。当$d_k$增大时，点积的方差也增大，这意味着点积的值会变得更大，分布也会更分散。**

**这解释了为什么当$d_k$较大时，点积的值会变得很大。因此，在计算点积后进行缩放（除以）$\sqrt{d_k}$可以减小点积的值，使得softmax函数的输入不会过于大，从而避免梯度消失的问题**。

不过，我还有一个疑问：为什么缩放因子是$\sqrt{d_k}$而不是$d_k$？或者说，为什么选择$\sqrt{d_k}$而不是其他形式的缩放？让我再仔细想想。

假设我们不进行缩放，直接计算点积，那么随着$\sqrt{d_k}$的增大，点积的值会增大，导致softmax函数的输出接近0或1，梯度变得很小，影响训练。如果我们进行缩放，比如除以$d_k$，那么点积的值会减小，但缩放因子是$\sqrt{d_k}$而不是$d_k$，这可能是**因为在计算点积时，每个分量的方差是1，而点积的方差是$d_k$，所以标准差是$\sqrt{d_k}$。因此，除以$\sqrt{d_k}$可以使点积的值在缩放后具有恒定的方差，从而保持数值的稳定性**。

另外，从计算效率的角度来看，**使用$\sqrt{d_k}$作为缩放因子可能在矩阵运算中更高效，因为矩阵乘法通常涉及平方根运算，而$\sqrt{d_k}$可以与矩阵运算中的其他操作结合，提高计算效率**。

总的来说，缩放点积注意力通过除以$\sqrt{d_k}$，可以**有效地控制点积的值，避免因维度增大而导致的梯度问题，同时保持数值稳定性，提高模型的训练效果**。

在Transformer模型中，缩放点积注意力（Scaled Dot-Product Attention）通过将点积结果除以√dk来解决维度dk较大时点积值过大的问题。以下是详细的解释：

1. **点积的性质**：
   - 假设查询向量$q$和键向量$k$的每个分量都是独立的随机变量，均值为0，方差为1。
   - 点积$q·k$的期望值为0，方差为$d_k$，其中$d_k$是向量的维度。
   - 随着$d_k$的增大，点积的方差增大，导致点积的值分布更分散，值的绝对值增大。
2. **缩放的必要性**：
   - 当$d_k$较大时，点积的值会变得很大，使得softmax函数的输入范围扩大。
   - softmax函数在输入值较大时，输出值接近0或1，导致梯度接近0，影响模型的训练效果。
   - 通过缩放点积结果，可以控制点积的值，避免因维度增大而导致的梯度问题。
3. **缩放因子的选择**：
   - 选择$\sqrt{d_k}$作为缩放因子，是因为点积的方差为$d_k$，标准差为$\sqrt{d_k}$。
   - 除以$\sqrt{d_k}$可以使缩放后的点积值具有恒定的方差，保持数值稳定性。
   - 这种缩放方式在矩阵运算中也更高效，因为矩阵乘法通常涉及平方根运算，√dk可以与矩阵运算中的其他操作结合，提高计算效率。
4. **实际效果**：
   - 缩放点积注意力通过除以$\sqrt{d_k}$，有效地控制了点积的值，避免了因维度增大而导致的梯度消失问题。
   - 这种方法在保持模型表达能力的同时，提高了训练的稳定性和效率，使得Transformer模型在各种任务中表现出色。

综上所述，缩放点积注意力中除以√$\sqrt{d_k}$是为了控制点积的值，避免因维度增大而导致的梯度问题，同时保持数值稳定性，提高模型的训练效果。



### 注意力可视化（Attention Visualizations Input-Input Lay  ）

图3展示了编码器中第五层自注意力机制如何捕捉长距离依赖关系的一个示例。许多注意力头关注到动词“making”与远处的“more difficult”之间的长距离依赖关系，从而完整地表达了“making...more difficult”这一短语。图中仅显示了“making”这个词的注意力分布，不同颜色代表不同的注意力头。建议彩色查看效果最佳。

![image-20250724174116122](http://image.huawei.com/tiny-lts/v1/images/hi3ms/e1b2a7dfa79ef4ea9a097b458503a01c_1768x1120.png)



图4展示了第六层中的两个注意力头，它们似乎参与了代词消解任务。顶部展示了第五个注意力头的完整注意力分布，而底部则单独展示了第五和第六个注意力头从单词“its”出发的注意力分布。值得注意的是，对于“its”这个词，注意力分布非常集中，表明注意力机制能够有效地捕捉到与该词相关的上下文信息，从而帮助模型理解代词指代关系。这种集中注意力的能力是Transformer模型在处理复杂语言任务时的一个显著优势。

![image-20250724174207087](http://image.huawei.com/tiny-lts/v1/images/hi3ms/90787c9380c147990584b4485ba1e1e3_1085x1170.png)

Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5 and 6. Note that the attentions are very sharp for this word.  

图5展示了编码器自注意力机制中，第六层中的第五层的两个不同注意力头的行为。这些注意力头表现出与句子结构相关的行为，具体来说：

1. **注意力头1**：这个注意力头主要关注句子中的主语和谓语之间的关系。例如，在句子“猫在睡觉”中，注意力头1会将“猫”与“睡觉”联系起来，帮助模型理解主语和谓语之间的关系。
2. **注意力头2**：这个注意力头则关注句子中的修饰语和被修饰语之间的关系。例如，在句子“红色的花很美丽”中，注意力头2会将“红色的”与“花”联系起来，帮助模型理解修饰语和被修饰语之间的关系。

通过这两个例子可以看出，不同的注意力头在模型中学习到了不同的任务，从而能够更好地理解和处理复杂的句子结构。这种分工合作的方式使得模型在处理自然语言时更加高效和准确。

![image-20250724174326957](http://image.huawei.com/tiny-lts/v1/images/hi3ms/8b6b6bb7d7b72b1802a86c865517f456_1046x1104.png)

Figure 5:    “Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks.”

### 缩放点积注意力示例

假设我们有以下矩阵：

- **查询矩阵 ( Q )**：维度为 ($ 2 \times 2$ )

$$
Q = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}
$$



- **键矩阵 ( K )**：维度为 ( $3 \times 2 $)

$$
K = \begin{bmatrix} 5 & 6 \\ 7 & 8 \\ 9 & 10 \end{bmatrix}
$$



- **值矩阵 ( V )**：维度为 ( $3 \times 2$ )

$$
V = \begin{bmatrix} 11 & 12 \\ 13 & 14 \\ 15 & 16 \end{bmatrix}
$$

**步骤 1：计算点积**

首先，计算查询矩阵 ( $Q$ ) 和键矩阵 ($ K$ ) 的转置 ( $K^T $) 的点积，维度为（$2 \times 3$）：
$$
Q \cdot K^T = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix} \cdot \begin{bmatrix} 5 & 7 & 9 \\ 6 & 8 & 10 \end{bmatrix} = \begin{bmatrix} 1 \times 5 + 2 \times 6 & 1 \times 7 + 2 \times 8 & 1 \times 9 + 2 \times 10 \\ 3 \times 5 + 4 \times 6 & 3 \times 7 + 4 \times 8 & 3 \times 9 + 4 \times 10 \end{bmatrix} = \begin{bmatrix} 17 & 23 & 29 \\ 39 & 53 & 67 \end{bmatrix}
$$


**步骤 2：缩放点积结果**

为了避免数值过大，对点积结果进行缩放。假设嵌入维度 ( $d = 2 $)，缩放因子为 ($ \sqrt{2} $)：
$$
\text{Scaled Attention Scores} = \frac{1}{\sqrt{2}} \times \begin{bmatrix} 17 & 23 & 29 \\ 39 & 53 & 67 \end{bmatrix} \approx \begin{bmatrix} 12.02 & 16.26 & 20.50 \\ 27.56 & 37.53 & 47.50 \end{bmatrix}
$$


**步骤 3：应用Softmax函数**

对缩放后的注意力分数矩阵应用Softmax函数，得到注意力权重矩阵。假设计算结果如下：
$$
\text{Attention Weights} = \begin{bmatrix} 0.2 & 0.3 & 0.5 \\ 0.1 & 0.4 & 0.5 \end{bmatrix}
$$


**步骤 4：加权求和**

最后，用注意力权重矩阵与值矩阵 ( V ) 进行矩阵乘法，得到最终的输出矩阵：
$$
\text{Output} = \begin{bmatrix} 0.2 & 0.3 & 0.5 \\ 0.1 & 0.4 & 0.5 \end{bmatrix} \cdot \begin{bmatrix} 11 & 12 \\ 13 & 14 \\ 15 & 16 \end{bmatrix} = \begin{bmatrix} 0.2 \times 11 + 0.3 \times 13 + 0.5 \times 15 & 0.2 \times 12 + 0.3 \times 14 + 0.5 \times 16 \\ 0.1 \times 11 + 0.4 \times 13 + 0.5 \times 15 & 0.1 \times 12 + 0.4 \times 14 + 0.5 \times 16 \end{bmatrix}
$$
计算具体数值：

- **第一行第一列**：

$$
0.2 \times 11 = 2.2 \\
0.3 \times 13 = 3.9 \\
0.5 \times 15 = 7.5 \\
\text{总和} = 2.2 + 3.9 + 7.5 = 13.6
$$



- **第一行第二列**：

$$
0.2 \times 12 = 2.4 \\
0.3 \times 14 = 4.2 \\
0.5 \times 16 = 8.0 \\
\text{总和} = 2.4 + 4.2 + 8.0 = 14.6
$$



- **第二行第一列**：

$$
0.1 \times 11 = 1.1 \\
0.4 \times 13 = 5.2 \\
0.5 \times 15 = 7.5 \\
\text{总和} = 1.1 + 5.2 + 7.5 = 13.8
$$



- **第二行第二列**：

$$
0.1 \times 12 = 1.2 \
0.4 \times 14 = 5.6 \
0.5 \times 16 = 8.0 \
\text{总和} = 1.2 + 5.6 + 8.0 = 14.8
$$

因此，最终的输出矩阵为：
$$
\text{Output} = \begin{bmatrix} 13.6 & 14.6 \\ 13.8 & 14.8 \end{bmatrix}
$$


### 缩放点积注意力机制的句子编码示例

为了更直观地理解缩放点积注意力机制，我们可以通过一个具体的句子编码示例来演示其工作原理。以下是详细的步骤说明：

**1. 准备输入句子**

假设我们有一个简单的句子：“猫在桌子上睡觉”。首先，将这个句子分解成词语列表：
$$
\text{词语列表} = ["猫", "在", "桌子", "上", "睡觉"]
$$
**2. 生成词语嵌入向量**

为每个词语生成一个嵌入向量。假设每个词语的嵌入向量维度为2：

- 猫：$[1, 2]$
- 在：$[3, 4]$
- 桌子：$[5, 6]$
- 上：$[7, 8]$
- 睡觉：$[9, 10]$

因此，输入嵌入矩阵 ( $X$ ) ，维度为 ($ 5 \times 2$ )：
$$
X = \begin{bmatrix}
1 & 2 \\
3 & 4 \\
5 & 6 \\
7 & 8 \\
9 & 10
\end{bmatrix}
$$
**3. 生成查询矩阵 ( Q )、键矩阵 ( K ) 和值矩阵 ( V )**

在缩放点积注意力机制中，查询、键和值矩阵通常通过线性变换从输入嵌入中得到。为了简化，我们假设 ( $Q$ )、( $K$ ) 和 ( $V$ ) 直接等于输入嵌入矩阵 ( $ X$ )：
$$
Q = K = V = X = \begin{bmatrix}
1 & 2 \\
3 & 4 \\
5 & 6 \\
7 & 8 \\
9 & 10
\end{bmatrix}
$$
**4. 计算点积**

计算查询矩阵 ( $Q$ ) 和键矩阵 ( $K$ ) 的点积，得到注意力分数矩阵，维度为 ($ 5 \times 5$ )：
$$
Q \cdot K^T = \begin{bmatrix}
1 & 2 \\
3 & 4 \\
5 & 6 \\
7 & 8 \\
9 & 10
\end{bmatrix}
\cdot
\begin{bmatrix}
1 & 3 & 5 & 7 & 9 \\
2 & 4 & 6 & 8 & 10
\end{bmatrix}
= \begin{bmatrix}
5 & 11 & 17 & 23 & 29 \\
11 & 25 & 39 & 53 & 67 \\
17 & 39 & 61 & 83 & 105 \\
23 & 53 & 83 & 113 & 143 \\
29 & 67 & 105 & 143 & 181
\end{bmatrix}
$$
**5. 缩放点积结果**

为了避免点积结果过大，对点积结果进行缩放。缩放因子为 ( $\sqrt{d}$ )，其中 ( $d$ ) 是查询和键的维度（这里是2），因此缩放因子为 ( $\sqrt{2} \approx 1.4142$ )。

缩放后的注意力分数矩阵，维度为 ($ 5 \times 5$)：
$$
\text{Scaled Attention Scores} = \frac{1}{1.4142} \times \begin{bmatrix}
5 & 11 & 17 & 23 & 29 \\
11 & 25 & 39 & 53 & 67 \\
17 & 39 & 61 & 83 & 105 \\
23 & 53 & 83 & 113 & 143 \\
29 & 67 & 105 & 143 & 181
\end{bmatrix} \approx \begin{bmatrix}
3.5355 & 7.7784 & 12.0260 & 16.2658 & 20.5029 \\
7.7784 & 17.6777 & 27.5637 & 37.5156 & 47.4342 \\
12.0260 & 27.5637 & 43.0143 & 58.6843 & 74.2258 \\
16.2658 & 37.5156 & 58.6843 & 79.8557 & 101.0890 \\
20.5029 & 47.4342 & 74.2258 & 101.0890 & 128.0369
\end{bmatrix}
$$
**6. 应用Softmax函数**

对缩放后的注意力分数矩阵应用Softmax函数，得到注意力权重矩阵。以第一行为例：
$$
\text{第一行} = [3.5355, 7.7784, 12.0260, 16.2658, 20.5029]
$$
计算每个元素的指数并求和：
$$
e^{3.5355} \approx 34.16, \quad e^{7.7784} \approx 2300.00, \quad e^{12.0260} \approx 162000.00, \quad e^{16.2658} \approx 9.00 \times 10^6, \quad e^{20.5029} \approx 5.00 \times 10^8
$$

$$
\text{总和} \approx 5.09 \times 10^8
$$
计算每个元素的概率：
$$
\text{第一行注意力权重} \approx [0.000000067, 0.00000452, 0.000318, 0.01768, 0.9825]
$$


**7. 加权求和**

用注意力权重矩阵对值矩阵 ( $V$ ) 进行加权求和，得到最终的输出矩阵。以第一行为例：
$$
\text{Output}_1 = 0.000000067 \times [1, 2] + 0.00000452 \times [3, 4] + 0.000318 \times [5, 6] + 0.01768 \times [7, 8] + 0.9825 \times [9, 10]
$$
计算结果：
$$
\text{Output}_1 \approx [8.967363627, 9.967363627]
$$
**8. 总结**

通过这个具体的句子编码示例，我们可以清晰地看到缩放点积注意力机制的工作流程：

1. **点积计算**：通过查询和键的点积，计算词语之间的相似性。
2. **缩放**：对点积结果进行缩放，防止数值过大。
3. **Softmax归一化**：将注意力分数转换为概率形式，得到注意力权重。
4. **加权求和**：根据注意力权重对值进行加权求和，生成最终的输出。

在这个示例中，**最后一个词语“睡觉”对其他词语的关注权重非常大，这表明模型主要关注最后一个词语。这可能是因为它的嵌入向量与其他词语的相似性较高。通过这样的机制，模型能够有效地捕捉到词语之间的关系，实现对输入序列中不同位置的关注**。

在实际应用中，嵌入向量的维度通常更高，且词语之间的相似性分布可能更加均匀。此外，多头注意力机制的引入可以进一步提高模型的表达能力，捕捉不同层次的语义信息。通过理解这个简单的句子编码示例，我们可以更好地掌握缩放点积注意力机制的核心原理及其在自然语言处理中的应用。



源码实现：

```python
class Attention(nn.Module):
    """
    Compute 'Scaled Dot Product Attention
    """

    def forward(self, query, key, value, mask=None, dropout=None):
        print("Attention query:", query)
        print("Attention key:", key)
        print("Attention value:", value)
    
        matmulQk = torch.matmul(query, key.transpose(-2, -1))
        sqrtQk = matmulQk / math.sqrt(query.size(-1))
        softmaxQk = F.softmax(sqrtQk, dim=-1)
        matmulQkv = torch.matmul(softmaxQk, value)

        print("Attention matmulQk:", matmulQk)
        print("Attention sqrtQk:", sqrtQk)
        print("Attention softmaxQk:", softmaxQk)
        print("Attention matmulQkv:", matmulQkv)
        return matmulQkv
```

测试结果：

```python
{'猫': tensor([1., 2.]), '在': tensor([3., 4.]), '桌子': tensor([5., 6.]), '上': tensor([7., 8.]), '睡觉': tensor([ 9., 10.])}
torch.Size([5, 2])
Attention query: tensor([[ 1.,  2.],
        [ 3.,  4.],
        [ 5.,  6.],
        [ 7.,  8.],
        [ 9., 10.]])
Attention key: tensor([[ 1.,  2.],
        [ 3.,  4.],
        [ 5.,  6.],
        [ 7.,  8.],
        [ 9., 10.]])
Attention value: tensor([[ 1.,  2.],
        [ 3.,  4.],
        [ 5.,  6.],
        [ 7.,  8.],
        [ 9., 10.]])
Attention matmulQk: tensor([[  5.,  11.,  17.,  23.,  29.],
        [ 11.,  25.,  39.,  53.,  67.],
        [ 17.,  39.,  61.,  83., 105.],
        [ 23.,  53.,  83., 113., 143.],
        [ 29.,  67., 105., 143., 181.]])
Attention sqrtQk: tensor([[  3.5355,   7.7782,  12.0208,  16.2635,  20.5061],
        [  7.7782,  17.6777,  27.5772,  37.4767,  47.3762],
        [ 12.0208,  27.5772,  43.1335,  58.6899,  74.2462],
        [ 16.2635,  37.4767,  58.6899,  79.9031, 101.1163],
        [ 20.5061,  47.3762,  74.2462, 101.1163, 127.9863]])
Attention softmaxQk: tensor([[4.2023e-08, 2.9245e-06, 2.0352e-04, 1.4163e-02, 9.8563e-01],
        [6.3503e-18, 1.2650e-13, 2.5199e-09, 5.0198e-05, 9.9995e-01],
        [9.4592e-28, 5.3937e-21, 3.0756e-14, 1.7537e-07, 1.0000e+00],
        [1.4089e-37, 2.2997e-28, 3.7536e-19, 6.1266e-10, 1.0000e+00],
        [0.0000e+00, 9.8050e-36, 4.5811e-24, 2.1403e-12, 1.0000e+00]])
Attention matmulQkv: tensor([[ 8.9708,  9.9708],
        [ 8.9999,  9.9999],
        [ 9.0000, 10.0000],
        [ 9.0000, 10.0000],
        [ 9.0000, 10.0000]])
```



### 多头注意力计算流程

1. **输入表示**：

   - 输入序列 ( $X$ ) 是一个 ( $n \times d$ ) 的矩阵，其中 ( $n$ ) 是序列长度，( $d$ ) 是嵌入维度。

2. **生成查询、键、值向量**：

   - 每个注意力头 ( $i$ ) 通过线性变换生成查询 ( $Q_i$ )、键 ( $K_i$ ) 和值 ( $V_i$ )：

   $$
   Q_i = X \cdot W_q^{(i)}, \quad K_i = X \cdot W_k^{(i)}, \quad V_i = X \cdot W_v^{(i)}
   $$


   ​	其中，( $W_q^{(i)}$ )、( $W_k^{(i)}$ )、( $W_v^{(i)}$ ) 是每个头的权重矩阵，维度为 ( $d \times d_k$ )，( $d_k = d/h$ )。

3. **计算注意力分数**：

   - 计算查询与键的点积，缩放因子为 ( $\sqrt{d_k}$ )：
     $$
     \text{Scores}_i = \frac{Q_i \cdot K_i^\top}{\sqrt{d_k}}
     $$

     得到 ( $n \times n$ ) 的分数矩阵。

4. **应用Softmax函数**：

   - 对分数矩阵的每一行应用Softmax，得到注意力权重：
     $$
     \text{Attention Weights}_i = \text{Softmax}(\text{Scores}_i)
     $$

5. **加权求和**：

   - 使用注意力权重对值向量加权求和：
     $$
     \text{Output}_i = \text{Attention Weights}_i \cdot V_i
     $$

6. **合并多个头的结果**：

   - 将所有 ( $h$ ) 个头的输出拼接：
     $$
     \text{Concatenated Output} = [\text{Output}_1, \text{Output}_2, \ldots, \text{Output}_h]
     $$
     拼接后的维度为 ($ n \times (h \cdot d_k) = n \times d $)。

7. **线性变换**：

   - 通过全连接层将拼接结果变换回原维度：
     $$
     \text{Final Output} = \text{Concatenated Output} \cdot W_o
     $$
     其中，( $W_o $) 是 ($ d \times d $) 的权重矩阵。

### 多头注意力机制的计算流程示例

为了更清晰地理解多头注意力机制，我们以一个具体的句子为例，逐步说明其计算流程。

**示例句子**

句子：“The cat sat on the mat.”

**步骤 1：词嵌入**

首先，将句子中的每个词转换为词嵌入向量。假设每个词嵌入的维度为 ( $d = 512$ )，句子长度为 ( $n = 6$ )（包括6个词）。

- 词嵌入矩阵 ( $X$ ) 的维度为 ( $6 \times 512$ )。

**步骤 2：生成查询、键、值向量**

对于每个注意力头 ( $i$ )（假设共有 ( $h = 8$ ) 个头），通过线性变换生成查询 ( $Q_i$ )、键 ( $K_i$ ) 和值 ( $V_i$ ) 向量。

- 每个线性变换的权重矩阵 ( $W_q^{(i)}$ )、( $W_k^{(i)}$ )、( $W_v^{(i)}$ ) 的维度为 ( $512 \times 64$ )（因为 ( $d_k = 512 / 8 = 64$ )）。

- 计算：
  $$
  Q_i = X \cdot W_q^{(i)} \quad (6 \times 64)
  $$

  $$
  K_i = X \cdot W_k^{(i)} \quad (6 \times 64)
  $$

  $$
  V_i = X \cdot W_v^{(i)} \quad (6 \times 64)
  $$

**步骤 3：计算注意力分数**

对于每个注意力头，计算查询与键的点积，并进行缩放。

- 点积计算：
  $$
  \text{Scores}_i = Q_i \cdot K_i^\top \quad (6 \times 6)
  $$

- 缩放：
  $$
  \text{Scores}_i = \frac{\text{Scores}_i}{\sqrt{64}} \quad (6 \times 6)
  $$

**步骤 4：应用Softmax函数**

对每个注意力头的分数矩阵的每一行应用Softmax函数，得到注意力权重。

- Softmax计算：
  $$
  \text{Attention Weights}_i = \text{Softmax}(\text{Scores}_i) \quad (6 \times 6)
  $$

**步骤 5：加权求和**

使用注意力权重对值向量进行加权求和，得到每个注意力头的输出。

- 加权求和：
  $$
  \text{Output}_i = \text{Attention Weights}_i \cdot V_i \quad (6 \times 64)
  $$

**步骤 6：合并多个头的结果**

将所有注意力头的输出拼接起来，得到一个 ( $6 \times 512$ ) 的矩阵。

- 拼接：
  $$
  \text{Concatenated Output} = [\text{Output}_1, \text{Output}_2, \ldots, \text{Output}_8] \quad (6 \times 512)
  $$

**步骤 7：线性变换**

通过全连接层将拼接后的输出变换回原维度。

- 线性变换：
  $$
  \text{Final Output} = \text{Concatenated Output} \cdot W_o \quad (6 \times 512)
  $$
  其中，( $W_o $) 是 ( $512 \times 512$ ) 的权重矩阵。

#### 总结

通过以上步骤，多头注意力机制能够并行计算多个注意力头，每个头捕捉输入序列中的不同特征。最终，通过拼接和线性变换，模型能够综合所有头的信息，生成更丰富的表示，从而增强对复杂关系的理解能力。

## 问题

### 1、点积注意力为啥有效？

点积注意力（Dot-Product Attention）的有效性主要体现在以下几个方面：

1. **捕捉相关性**：通过计算查询（Query）和键（Key）之间的点积，点积注意力能够有效地捕捉到两者之间的相关性。点积越大，表示两者越相似，从而在注意力机制中给予更高的权重。
2. **自适应权重分配**：应用Softmax函数后，点积结果被转化为概率分布，使得不同位置的信息能够根据其相关性动态地分配注意力权重。这种自适应的权重分配机制使得模型能够聚焦于重要的信息。
3. **高效计算**：点积操作可以在矩阵层面上进行并行计算，这对于处理大规模数据和加速模型训练非常重要。这种高效的计算方式使得点积注意力在实际应用中具有很好的可扩展性。
4. **灵活性**：点积注意力机制本身具有较高的灵活性，可以与其他神经网络结构（如Transformer）结合使用，适应不同的任务需求。这种灵活性使得点积注意力在各种自然语言处理任务中表现出色。
5. **模型性能提升**：在Transformer模型中，点积注意力机制能够有效地捕捉到不同位置之间的依赖关系，从而提升模型的性能。通过选择性地关注重要的信息，模型能够更好地理解和生成文本。

尽管点积注意力具有诸多优势，但也存在一些局限性。例如，当处理长序列时，计算注意力权重的复杂度会变得很高，导致计算资源的消耗过大。为了解决这一问题，一些改进的方法，如稀疏注意力机制或局部注意力机制，被提出以降低计算复杂度。

综上所述，点积注意力的有效性主要来自于其能够通过点积计算捕捉到查询和键之间的相关性，并通过Softmax函数实现自适应的注意力权重分配，从而实现信息的聚焦和选择性关注。这种机制在许多自然语言处理任务中表现出色，但也有一些局限性需要通过其他方法来弥补。

点积计算向量相似度的步骤如下：

1. **向量表示**：首先，将需要比较的两个对象（如文本、图像等）表示为向量。这些向量通常是在高维空间中，每个维度代表一个特征或属性。
2. **计算点积**：使用点积公式计算两个向量的点积。点积的结果是一个标量值，反映了两个向量在方向上的相似程度。
3. **归一化处理**：为了消除向量长度对相似度的影响，通常会对点积结果进行归一化处理。这可以通过将点积除以两个向量模长的乘积来实现，即计算余弦相似度。
4. **解释相似度**：余弦相似度的范围在-1到1之间。值越接近1，表示两个向量方向越接近，相似度越高；值越接近-1，表示方向越相反，相似度越低；值为0表示两个向量垂直，没有相似性。
5. **应用相似度**：根据计算得到的相似度值，可以用于各种应用，如文本分类、推荐系统、图像检索等，以衡量不同对象之间的相似程度。

通过以上步骤，点积计算向量相似度提供了一种有效的方法，帮助我们在高维空间中量化和比较不同对象的相似性。

